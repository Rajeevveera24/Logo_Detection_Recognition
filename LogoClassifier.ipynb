{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logoclassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjungoel7/Logo_Detection_Recognition/blob/master/LogoClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi-MHDZ4UE-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76Q5A3UEUNiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd drive/My\\ Drive/Deep\\ Learning/qmul_toplogo10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFToW1JdUjIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRDn7E5QUko0",
        "colab_type": "code",
        "outputId": "004167f5-728d-4c25-dde0-2308e21ddb2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "dir = \"masks\"\n",
        "temp = os.path.join(str(dir))\n",
        "\n",
        "boxes = np.zeros((700, 4))\n",
        "cnt = 0\n",
        "\n",
        "for temp_dir_name in sorted(os.listdir(temp)):\n",
        "\n",
        "    print(temp_dir_name)\n",
        "    tmp = os.path.join(str(dir) + '/' + str(temp_dir_name))\n",
        "\n",
        "    for fil in sorted(os.listdir(tmp)):\n",
        "      f = open(str(dir) + '/' + str(temp_dir_name) + '/' + str(fil), \"r\")\n",
        "      \n",
        "      contents = f.read()\n",
        "      # print(contents)\n",
        "\n",
        "      w1 = int(contents.split()[0])\n",
        "      h1 = int(contents.split()[1])\n",
        "      w2 = int(contents.split()[2]) + w1\n",
        "      h2 = int(contents.split()[3]) + h1\n",
        "      # print(str(fil), w1, h1, w2, h2)\n",
        "\n",
        "      boxes[cnt][0] = w1\n",
        "      boxes[cnt][1] = h1\n",
        "      boxes[cnt][2] = w2\n",
        "      boxes[cnt][3] = h2\n",
        "\n",
        "      cnt += 1\n",
        "\n",
        "print(type(boxes))\n",
        "\n",
        "# boxes = "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adidas0\n",
            "chanel\n",
            "gucci\n",
            "hh\n",
            "lacoste\n",
            "mk\n",
            "nike\n",
            "prada\n",
            "puma\n",
            "supreme\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7QWvXnn-ZCe",
        "colab_type": "code",
        "outputId": "a812bcbf-559d-4c06-e85c-a770e32861fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "print(boxes)\n",
        "boxes = np.reshape(boxes, (700, 4))\n",
        "np.save('boxes', boxes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[567. 408. 730. 563.]\n",
            " [616.   7. 774. 118.]\n",
            " [283.  69. 369. 127.]\n",
            " ...\n",
            " [159. 226. 425. 339.]\n",
            " [293. 163. 368. 189.]\n",
            " [153. 160. 466. 267.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYnRdfsL_7dB",
        "colab_type": "code",
        "outputId": "d715e872-a478-4978-90ca-5cd1fd0aa96b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(boxes.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(700, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CphBrBkr6tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "labels = np.zeros(700, dtype = 'uint8')\n",
        "images = np.zeros((700,200,200,3), dtype = 'uint8')\n",
        "cnt = 0\n",
        "\n",
        "dir = \"jpg\"\n",
        "temp = os.path.join(str(dir))\n",
        "\n",
        "for temp_dir_name in sorted(os.listdir(temp)):\n",
        "\n",
        "    print(temp_dir_name)\n",
        "    tmp = os.path.join(str(dir) + '/' + str(temp_dir_name))\n",
        "\n",
        "    # print(sorted(os.listdir(tmp)))\n",
        "\n",
        "    for image in sorted(os.listdir(tmp)):\n",
        "      print(image)\n",
        "      if image.endswith(\".jpg\") or image.endswith(\".png\"):\n",
        "        im = cv2.imread(str(dir) + '/' + str(temp_dir_name) + '/' + str(image))\n",
        "\n",
        "        box = boxes[cnt]\n",
        "        print(box)\n",
        "        box = box.reshape((1,4))\n",
        "        w1 = int(box[0][0])\n",
        "        h1 = int(box[0][1])\n",
        "        w2 = int(box[0][2])\n",
        "        h2 = int(box[0][3])\n",
        "        print(w1, h1, w2, h2)\n",
        "        \n",
        "        # print(im.shape)\n",
        "        # im = np.array(im)\n",
        "        # print(im)\n",
        "\n",
        "        tmpImg = np.zeros((h2 - h1, w2 - w1, 3))\n",
        "\n",
        "        tmpImg = im[h1:h2, w1:w2, :]\n",
        "        \n",
        "        # print(tmpImg.shape)\n",
        "\n",
        "        tmpImg = cv2.resize(tmpImg, (200, 200), interpolation = cv2.INTER_AREA)\n",
        "        \n",
        "        images[cnt] = tmpImg\n",
        "\n",
        "        cnt += 1\n",
        "        \n",
        "      #   contents = f.read()\n",
        "      #   # print(contents)\n",
        "      #   w1 = int(contents.split()[0])\n",
        "      #   w2 = int(contents.split()[2]) + w1\n",
        "      #   h1 = int(contents.split()[1])\n",
        "      #   h2 = int(contents.split()[3]) + h1\n",
        "      #   print(str(fil), w1, w2, h1, h2)\n",
        "  # print(temp_file_names)\n",
        "  # for fil in temp_file_names:\n",
        "  #     f = open(str(fil), \"r\")\n",
        "  #     contents = f.read()\n",
        "  #     # print(contents)\n",
        "  #     w1 = int(contents.split()[0])\n",
        "  #     w2 = int(contents.split()[2]) + w1\n",
        "  #     h1 = int(contents.split()[1])\n",
        "  #     h2 = int(contents.split()[3]) + h1\n",
        "  #     print( w1, w2, h1, h2)\n",
        "  #     # print(x1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NunT5pBVtNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_labels = np.zeros((700,10))\n",
        "lbl = -1\n",
        "for i in range(0,700):\n",
        "  if i%70 == 0:\n",
        "    lbl = lbl + 1\n",
        "  target_labels[i][lbl] = 1\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIeX-YL9ugSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "images, final_labels = shuffle(images, target_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu5jjBBsw4ff",
        "colab_type": "code",
        "outputId": "42237ba2-4292-4560-ddb9-c676bd7dbbe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "cnn_model = Sequential()\n",
        "\n",
        "#after the input layer, add the first convolutional layer with 32 2x2-filters \n",
        "cnn_model.add(Conv2D (kernel_size = (2,2), filters = 32, \n",
        "                      input_shape=X_train.shape[1:], activation='relu'))\n",
        "#add a max pooling layer with a 2x2 pooling window\n",
        "cnn_model.add(MaxPooling2D(pool_size=2))\n",
        "#add the second convolutional layer with 64 2x2-filters \n",
        "cnn_model.add(Conv2D(kernel_size = 2, filters = 64, activation='relu'))\n",
        "cnn_model.add(MaxPooling2D(pool_size=2))\n",
        "#add the third convolutional layer with 128 2x2-filters \n",
        "cnn_model.add(Conv2D(kernel_size = 2, filters = 128, activation='relu'))\n",
        "#add a dropout layer so that each node has a chance of 20% to be dropped when training\n",
        "cnn_model.add(Dropout(0.2))\n",
        "cnn_model.add(MaxPooling2D(pool_size = 2))\n",
        "#add a global average pooling layer\n",
        "cnn_model.add(GlobalAveragePooling2D())\n",
        "#add the final fully connected output layer with 109 node for all 109 logo classes\n",
        "cnn_model.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "cnn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_49 (Conv2D)           (None, 199, 199, 32)      416       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_49 (MaxPooling (None, 99, 99, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 98, 98, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_50 (MaxPooling (None, 49, 49, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 48, 48, 128)       32896     \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_51 (MaxPooling (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_17  (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 42,858\n",
            "Trainable params: 42,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Up15yeixp1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, final_labels, test_size=0.33)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMiAb716xhHI",
        "colab_type": "code",
        "outputId": "07742fdc-28ee-4e87-88ab-4c336bceef83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Compile the model \n",
        "from keras.optimizers import adam\n",
        "\n",
        "myopt = adam(lr=0.0001, decay=1e-6)\n",
        "\n",
        "cnn_model.compile(optimizer=myopt, \n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "#Train the model for 100 epochs\n",
        "H = cnn_model.fit(X_train, y_train, epochs=100, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 8.2541 - acc: 0.1343\n",
            "Epoch 2/100\n",
            "469/469 [==============================] - 0s 816us/step - loss: 6.4479 - acc: 0.1770\n",
            "Epoch 3/100\n",
            "469/469 [==============================] - 0s 805us/step - loss: 4.5851 - acc: 0.2175\n",
            "Epoch 4/100\n",
            "469/469 [==============================] - 0s 813us/step - loss: 3.7834 - acc: 0.1748\n",
            "Epoch 5/100\n",
            "469/469 [==============================] - 0s 815us/step - loss: 2.8435 - acc: 0.2090\n",
            "Epoch 6/100\n",
            "469/469 [==============================] - 0s 822us/step - loss: 2.3383 - acc: 0.2260\n",
            "Epoch 7/100\n",
            "469/469 [==============================] - 0s 823us/step - loss: 2.1908 - acc: 0.2687\n",
            "Epoch 8/100\n",
            "469/469 [==============================] - 0s 807us/step - loss: 2.1339 - acc: 0.2623\n",
            "Epoch 9/100\n",
            "469/469 [==============================] - 0s 820us/step - loss: 2.0575 - acc: 0.2580\n",
            "Epoch 10/100\n",
            "469/469 [==============================] - 0s 812us/step - loss: 2.0250 - acc: 0.2985\n",
            "Epoch 11/100\n",
            "469/469 [==============================] - 0s 820us/step - loss: 2.0086 - acc: 0.2729\n",
            "Epoch 12/100\n",
            "469/469 [==============================] - 0s 829us/step - loss: 1.9586 - acc: 0.2793\n",
            "Epoch 13/100\n",
            "469/469 [==============================] - 0s 811us/step - loss: 1.9005 - acc: 0.3198\n",
            "Epoch 14/100\n",
            "469/469 [==============================] - 0s 819us/step - loss: 1.9023 - acc: 0.3049\n",
            "Epoch 15/100\n",
            "469/469 [==============================] - 0s 822us/step - loss: 1.8635 - acc: 0.3284\n",
            "Epoch 16/100\n",
            "469/469 [==============================] - 0s 810us/step - loss: 1.8710 - acc: 0.3198\n",
            "Epoch 17/100\n",
            "469/469 [==============================] - 0s 827us/step - loss: 1.8421 - acc: 0.3305\n",
            "Epoch 18/100\n",
            "469/469 [==============================] - 0s 832us/step - loss: 1.8122 - acc: 0.3369\n",
            "Epoch 19/100\n",
            "469/469 [==============================] - 0s 811us/step - loss: 1.8136 - acc: 0.3646\n",
            "Epoch 20/100\n",
            "469/469 [==============================] - 0s 832us/step - loss: 1.7616 - acc: 0.4009\n",
            "Epoch 21/100\n",
            "469/469 [==============================] - 0s 809us/step - loss: 1.7606 - acc: 0.3731\n",
            "Epoch 22/100\n",
            "469/469 [==============================] - 0s 823us/step - loss: 1.7488 - acc: 0.3731\n",
            "Epoch 23/100\n",
            "469/469 [==============================] - 0s 816us/step - loss: 1.7493 - acc: 0.3539\n",
            "Epoch 24/100\n",
            "469/469 [==============================] - 0s 819us/step - loss: 1.6940 - acc: 0.3817\n",
            "Epoch 25/100\n",
            "469/469 [==============================] - 0s 815us/step - loss: 1.6891 - acc: 0.4094\n",
            "Epoch 26/100\n",
            "469/469 [==============================] - 0s 809us/step - loss: 1.7140 - acc: 0.3945\n",
            "Epoch 27/100\n",
            "469/469 [==============================] - 0s 825us/step - loss: 1.6625 - acc: 0.4328\n",
            "Epoch 28/100\n",
            "469/469 [==============================] - 0s 824us/step - loss: 1.6769 - acc: 0.4115\n",
            "Epoch 29/100\n",
            "469/469 [==============================] - 0s 804us/step - loss: 1.6556 - acc: 0.4030\n",
            "Epoch 30/100\n",
            "469/469 [==============================] - 0s 822us/step - loss: 1.6920 - acc: 0.3966\n",
            "Epoch 31/100\n",
            "469/469 [==============================] - 0s 823us/step - loss: 1.6204 - acc: 0.4478\n",
            "Epoch 32/100\n",
            "469/469 [==============================] - 0s 812us/step - loss: 1.6729 - acc: 0.4030\n",
            "Epoch 33/100\n",
            "469/469 [==============================] - 0s 834us/step - loss: 1.6634 - acc: 0.4009\n",
            "Epoch 34/100\n",
            "469/469 [==============================] - 0s 812us/step - loss: 1.6609 - acc: 0.3859\n",
            "Epoch 35/100\n",
            "469/469 [==============================] - 0s 812us/step - loss: 1.6469 - acc: 0.3987\n",
            "Epoch 36/100\n",
            "469/469 [==============================] - 0s 824us/step - loss: 1.6837 - acc: 0.3838\n",
            "Epoch 37/100\n",
            "469/469 [==============================] - 0s 834us/step - loss: 1.5749 - acc: 0.4691\n",
            "Epoch 38/100\n",
            "469/469 [==============================] - 0s 854us/step - loss: 1.6178 - acc: 0.4115\n",
            "Epoch 39/100\n",
            "469/469 [==============================] - 0s 811us/step - loss: 1.5655 - acc: 0.4286\n",
            "Epoch 40/100\n",
            "469/469 [==============================] - 0s 822us/step - loss: 1.5570 - acc: 0.4414\n",
            "Epoch 41/100\n",
            "469/469 [==============================] - 0s 831us/step - loss: 1.5327 - acc: 0.4584\n",
            "Epoch 42/100\n",
            "469/469 [==============================] - 0s 798us/step - loss: 1.5252 - acc: 0.4478\n",
            "Epoch 43/100\n",
            "469/469 [==============================] - 0s 838us/step - loss: 1.5343 - acc: 0.4755\n",
            "Epoch 44/100\n",
            "469/469 [==============================] - 0s 818us/step - loss: 1.5157 - acc: 0.4691\n",
            "Epoch 45/100\n",
            "469/469 [==============================] - 0s 811us/step - loss: 1.5043 - acc: 0.4606\n",
            "Epoch 46/100\n",
            "469/469 [==============================] - 0s 809us/step - loss: 1.5076 - acc: 0.4606\n",
            "Epoch 47/100\n",
            "469/469 [==============================] - 0s 809us/step - loss: 1.5404 - acc: 0.4520\n",
            "Epoch 48/100\n",
            "469/469 [==============================] - 0s 820us/step - loss: 1.5658 - acc: 0.4648\n",
            "Epoch 49/100\n",
            "469/469 [==============================] - 0s 821us/step - loss: 1.5254 - acc: 0.4456\n",
            "Epoch 50/100\n",
            "469/469 [==============================] - 0s 814us/step - loss: 1.5395 - acc: 0.4307\n",
            "Epoch 51/100\n",
            "469/469 [==============================] - 0s 829us/step - loss: 1.4834 - acc: 0.4989\n",
            "Epoch 52/100\n",
            "469/469 [==============================] - 0s 819us/step - loss: 1.4697 - acc: 0.4840\n",
            "Epoch 53/100\n",
            "469/469 [==============================] - 0s 816us/step - loss: 1.5001 - acc: 0.4414\n",
            "Epoch 54/100\n",
            "469/469 [==============================] - 0s 842us/step - loss: 1.4923 - acc: 0.4648\n",
            "Epoch 55/100\n",
            "469/469 [==============================] - 0s 815us/step - loss: 1.4622 - acc: 0.4691\n",
            "Epoch 56/100\n",
            "469/469 [==============================] - 0s 820us/step - loss: 1.4597 - acc: 0.4883\n",
            "Epoch 57/100\n",
            "469/469 [==============================] - 0s 821us/step - loss: 1.4407 - acc: 0.4904\n",
            "Epoch 58/100\n",
            "469/469 [==============================] - 0s 809us/step - loss: 1.4731 - acc: 0.4712\n",
            "Epoch 59/100\n",
            "469/469 [==============================] - 0s 832us/step - loss: 1.4283 - acc: 0.4883\n",
            "Epoch 60/100\n",
            "469/469 [==============================] - 0s 814us/step - loss: 1.4337 - acc: 0.4883\n",
            "Epoch 61/100\n",
            "469/469 [==============================] - 0s 798us/step - loss: 1.4370 - acc: 0.4797\n",
            "Epoch 62/100\n",
            "469/469 [==============================] - 0s 819us/step - loss: 1.3943 - acc: 0.5096\n",
            "Epoch 63/100\n",
            "469/469 [==============================] - 0s 813us/step - loss: 1.4338 - acc: 0.4904\n",
            "Epoch 64/100\n",
            "469/469 [==============================] - 0s 817us/step - loss: 1.3773 - acc: 0.5245\n",
            "Epoch 65/100\n",
            "469/469 [==============================] - 0s 806us/step - loss: 1.4018 - acc: 0.5224\n",
            "Epoch 66/100\n",
            "469/469 [==============================] - 0s 808us/step - loss: 1.4429 - acc: 0.5032\n",
            "Epoch 67/100\n",
            "469/469 [==============================] - 0s 806us/step - loss: 1.3882 - acc: 0.4989\n",
            "Epoch 68/100\n",
            "469/469 [==============================] - 0s 813us/step - loss: 1.3681 - acc: 0.5117\n",
            "Epoch 69/100\n",
            "469/469 [==============================] - 0s 815us/step - loss: 1.3476 - acc: 0.5373\n",
            "Epoch 70/100\n",
            "469/469 [==============================] - 0s 819us/step - loss: 1.3814 - acc: 0.5203\n",
            "Epoch 71/100\n",
            "469/469 [==============================] - 0s 798us/step - loss: 1.3667 - acc: 0.5096\n",
            "Epoch 72/100\n",
            "469/469 [==============================] - 0s 787us/step - loss: 1.3353 - acc: 0.5522\n",
            "Epoch 73/100\n",
            "469/469 [==============================] - 0s 812us/step - loss: 1.3649 - acc: 0.5267\n",
            "Epoch 74/100\n",
            "469/469 [==============================] - 0s 807us/step - loss: 1.3858 - acc: 0.4968\n",
            "Epoch 75/100\n",
            "469/469 [==============================] - 0s 807us/step - loss: 1.3661 - acc: 0.5224\n",
            "Epoch 76/100\n",
            "469/469 [==============================] - 0s 815us/step - loss: 1.3451 - acc: 0.5309\n",
            "Epoch 77/100\n",
            "469/469 [==============================] - 0s 810us/step - loss: 1.3413 - acc: 0.5565\n",
            "Epoch 78/100\n",
            "469/469 [==============================] - 0s 812us/step - loss: 1.3600 - acc: 0.5139\n",
            "Epoch 79/100\n",
            "469/469 [==============================] - 0s 805us/step - loss: 1.3061 - acc: 0.5458\n",
            "Epoch 80/100\n",
            "469/469 [==============================] - 0s 807us/step - loss: 1.3288 - acc: 0.5522\n",
            "Epoch 81/100\n",
            "469/469 [==============================] - 0s 816us/step - loss: 1.3189 - acc: 0.5394\n",
            "Epoch 82/100\n",
            "469/469 [==============================] - 0s 816us/step - loss: 1.3192 - acc: 0.5437\n",
            "Epoch 83/100\n",
            "469/469 [==============================] - 0s 829us/step - loss: 1.3294 - acc: 0.5544\n",
            "Epoch 84/100\n",
            "469/469 [==============================] - 0s 813us/step - loss: 1.2893 - acc: 0.5586\n",
            "Epoch 85/100\n",
            "469/469 [==============================] - 0s 815us/step - loss: 1.3227 - acc: 0.5245\n",
            "Epoch 86/100\n",
            "469/469 [==============================] - 0s 814us/step - loss: 1.3074 - acc: 0.5480\n",
            "Epoch 87/100\n",
            "469/469 [==============================] - 0s 803us/step - loss: 1.3121 - acc: 0.5309\n",
            "Epoch 88/100\n",
            "469/469 [==============================] - 0s 812us/step - loss: 1.2941 - acc: 0.5480\n",
            "Epoch 89/100\n",
            "469/469 [==============================] - 0s 833us/step - loss: 1.2994 - acc: 0.5565\n",
            "Epoch 90/100\n",
            "469/469 [==============================] - 0s 845us/step - loss: 1.2838 - acc: 0.5672\n",
            "Epoch 91/100\n",
            "469/469 [==============================] - 0s 812us/step - loss: 1.2867 - acc: 0.5608\n",
            "Epoch 92/100\n",
            "469/469 [==============================] - 0s 820us/step - loss: 1.3346 - acc: 0.5416\n",
            "Epoch 93/100\n",
            "469/469 [==============================] - 0s 815us/step - loss: 1.2757 - acc: 0.5608\n",
            "Epoch 94/100\n",
            "469/469 [==============================] - 0s 823us/step - loss: 1.2896 - acc: 0.5501\n",
            "Epoch 95/100\n",
            "469/469 [==============================] - 0s 799us/step - loss: 1.2756 - acc: 0.5629\n",
            "Epoch 96/100\n",
            "469/469 [==============================] - 0s 809us/step - loss: 1.3329 - acc: 0.5203\n",
            "Epoch 97/100\n",
            "469/469 [==============================] - 0s 827us/step - loss: 1.3139 - acc: 0.5352\n",
            "Epoch 98/100\n",
            "469/469 [==============================] - 0s 812us/step - loss: 1.2643 - acc: 0.5544\n",
            "Epoch 99/100\n",
            "469/469 [==============================] - 0s 817us/step - loss: 1.2775 - acc: 0.5394\n",
            "Epoch 100/100\n",
            "469/469 [==============================] - 0s 820us/step - loss: 1.2976 - acc: 0.5288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYoqBhea1Jya",
        "colab_type": "code",
        "outputId": "57885504-4533-438b-bad9-8ee8fa2df989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "cnn_model.evaluate(X_test,y_test, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "231/231 [==============================] - 1s 3ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.7983300691043145, 0.4805194812935668]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    }
  ]
}